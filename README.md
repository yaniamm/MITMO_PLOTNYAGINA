# lab1
лабораторная 1. Машинное обучение
Плотнягина Яна ПМИ 3-2 Вариант 13

Лабораторная работа №1: Оценка точности модели снепрерывной зависимой переменной В практических примерах ниже показано: как делить данные на выборки (обучающую и тестовую); как считать MSE: среднеквадратическую ошибку модели; как меняются MSE на тестовой и обучающей выборках с изменением гибкости (числа степеней свободы) модели. Модели: сглаживающие сплайны. Данные: сгенерированные.

Задача 1: Решение
1. зададим исходные данные и функцию и построим по ним обучающую и тестирующую выборки. Представим эти данные на графике
2. С помощью библиотеки R сплайны на обуч. выборки для DF=38 и проверим MSE значение для обучающей и тестирующей выборок. Получим сплайн на графике.
3. Подставим разные значения DF от 2 до 40 и найдем такое значение, при котором MSE значение будет наименьшим. Представим все на графике.


Результат:

1. При подборе разных значений DF определили, что для функции f(X) = 25 + 0.02 * x − 0.003 ⋅*(x − 45) + 0.00006*(x-54)^3 лучшее значение df=6

2. На графике видно, что MSE на тестовой выборке начинает уменьшаться и затем стабилизируется при увеличении количества степеней свободы. После некоторого значения, MSE на тестовой выборке начинает расти, что может свидетельствовать о переобучении модели.
Исходя из графика, можно сделать вывод, что наилучшим количеством степеней свободы для данной модели является значение около 6 . Это значение обеспечивает минимальное значение MSE на тестовой выборке и, следовательно, лучшую обобщающую способность модели.


Задача 2: Решение

1. Потворяем перебор df значение от 2 до 40 и разные значения n_all[450,400,350]. Полученные значения сохраним в фрейм и выберем для каждого n_all такое df, чтобы тестовое MSE было наименьшим. Представим график обучающих и тестовых MSE для разных df и для разных n_all, выделяя точки с наименьшим MSE.


Результат:

Среднеквадратичная ошибка (MSE) уменьшается с увеличением количества наблюдений в модели. Это связано с тем, что с увеличением размера выборки увеличивается точность оценок параметров модели, что в свою очередь приводит к уменьшению ошибки модели. Однако в некоторых случаях при увеличении выборки MSE может оставаться примерно постоянным или даже увеличиваться, если новые наблюдения не добавляют дополнительной информации или являются не репрезентативными.
Важно заметить при количестве наблюдений 450 MSE_train = 0.920065	MSE_test = 1.094269

при 400 MSE_train = 1.035611 	MSE_test = 0.848195

при 350 MSE_train = 1.025383  	MSE_test = 0.750627 
то есть можно сделать вывод о том, что при уменьшении количества наблюдений  значение MSE_test уменьшается

Набор данных с 450 наблюдениями:

Графики MSE для обучающей и тестовой выборок показывают сходную форму. При увеличении количества степеней свободы, MSE на обеих выборках сначала снижается, затем после некоторого порога начинает расти для тестовой выборки, в то время как MSE для обучающей выборки продолжает снижаться. Это свидетельствует о переобучении модели на более сложных сплайнах, когда количество степеней свободы становится слишком высоким.

Набор данных с 400 наблюдениями:

Графики MSE для обучающей и тестовой выборок также показывают сходную форму, как и в предыдущем случае. Однако можно заметить, что MSE для обеих выборок начинает расти при более низких значениях степеней свободы по сравнению с набором данных с 450 наблюдениями. Это может быть связано с тем, что меньшее количество данных требует менее сложных моделей для адекватного предсказания.

Набор данных с 350 наблюдениями:

В данном случае графики MSE также показывают сходную форму, но уровень MSE для обеих выборок в целом выше, что может быть связано с меньшим объемом данных. При этом тренд роста MSE для тестовой выборки начинается на более низких значениях степеней свободы, чем для наборов данных с большим количеством наблюдений, что указывает на более высокий риск переобучения модели.

--------------------

# lab2
Лабараторная работа №2 Линейные модели. Кросс-валидация.

Вариант №13

Ниже показано:

как пользоваться инструментами предварительного анализа для поиска линейных взаимосвязей;

как строить и интерпретировать линейные модели с логарифмами;

как оценивать точность моделей с перекрёстной проверкой (LOOCV, проверка по блокам).

Модели: множественная линейная регрессия

Данные: https://raw.githubusercontent.com/ania607/ML/main/data/Carseats.csv

Зависимая переменная - Sales / продажа (в тысячах штук) в каждом магазине

Независимые переменные:

Price / цены компании на автокресла в каждом магазине;
CompPrice / цена конкурента в
каждом магазине;
ShelveLoc / качество стеллажа для размещения автокресел в каждом магазине: Плохое, Хорошее и Среднее.

Проверка осуществляется методом LOOCV

Задачи:

1 Данные своего варианта (см. таблицу ниже) разделить на выборку для построения
моделей (80%) и отложенные наблюдения (20%). Оставить в таблице только указанные в
варианте переменные. Отложенные наблюдения использовать только в задании 6

2 Провести предварительный анализ данных с помощью описательных статистик и
графиков, оценить взаимосвязь.

3 Проверить Y на нормальность. Если он распределён не по нормальному закону,
прологарифмировать и снова провести анализ взаимосвязей переменных.

4 Составить список возможных спецификаций моделей множественной регрессии (на
исходной Y и на логарифме Y ).

5 Оценить параметры моделей из списка. Оценить точность моделей методом
перекрёстной проверки, указанным в варианте. Найти самую точную из моделей для Y .
Найти самую точную из моделей для log(Y ).

6 Сделать прогноз с помощью самых точных моделей на отложенные наблюдения.
Рассчитать MSEtest вручную и выбрать одну наиболее точную модель.
Проинтерпретировать её параметры.


Задача 1:
Загружаем все необходимые для работы модули, определяем константы и загружаем входные данные. Во входных данных фильтруем только необходимые нам для работы признаки. Определяем фиктивные переменные для качественного признака и объединяем их с исходными данными. Также разделяем исходную выборку на тренировочную и тестовую.

Результат задачи 1:
Получили две выборки: тестовая и тренировочная

Задача 2:

Проведем анализ на тренировочной выборке:
1. построим гистограммы распределения и графики зависимости количественных переменных;
2. построим гистограммы распределения и графики зависимости количественных перемнных для разных значений качественной перемнной;
3. построим матрицу корреляции количественных переменных;
4. построим матрицу корреляции количественных переменных для разных значений качественной переменной.

Результаты задачи 2:

1. По гистограмме распределения "Sales" можно сделать предположение, что данные распределены нормально.

2. По графикам распределения видно, что переменная "Sales" слабо зависит от "CompPrice" и сильно обратно зависима от переменной "Price". Это подтверждается матрицей корреляции.

3. Разные значения качественной переменной "ShelveLoc" заметно влияют на среднее значения распределения переменной "Sales".

По матрицам корреляции видно, что значение переменной "ShelveLoc" == "Good" усиливает коррелированность "Price" и "Sales", а значение переменной "ShelveLoc" == "Medium" усиливает коррелированность "CompPrice" и "Sales".

Задача 3:
Проверим распределение переменной "Sales" на нормальность при помощи теста Шапиро-Уилка. Если распределение не является нормальным, проверим логарифм переменной на нормальность.

Результаты задачи 3:
Так как исходные данные обладают значениями "Sales" == 0, а также так как мы уже подтвердили нормальность переменной "Sales", логарифм переменной "Sales" далее рассматриваться не будет.

Задача 4 решение и резульаты:
в соответсвие с резульатами задачи 2 можно составить следующие линейные спецификации модели:

Sales = CompPrice + Price + Medium + Good
Sales = CompPrice + Price * Good + Medium + Good
Sales = CompPrice * Medium + Price + Medium + Good
Sales = CompPrice * Medium + Price * Good + Medium + Good

показательные модели рассматриваться не будут (в соответсвии с результатами задачи 3) 

Задача 5:
Построим модели, определенные в задании 4.

Определим параметры моделей и оценим их точность методом LOOCV.

Найдем самую точную из моделей.

Результаты задачи 5:
Мы определили, что наименьшую ошибку методом LOOCV выдает модель №1:

Sales = CompPrice + Price + Medium + Good


Задача 6:
Сделаем прогноз первой моделью на тестовые данные и оценим полученную ошибку тестирования. Оценим среднюю ошибку модели.

Построим первую модель на всех данных и проинтерпретируем ее параметры.

Результаты задачи 6:

Ошибка модели, построенной на тренировочных данных составила 21.8% от среднего значения Y

В результате построения на всех данных была получена модель:

Sales = 0.09 * CompPrice - 0.091 * Price + 4.862 * Good + 1.812 * Medium

Ее параметры можно интерпретировать следующим образом:

compprice (цена конкурента): Коэффициент перед этой переменной равен 0.09. Это означает, что при увеличении цены конкурента на единицу, продажи автокресел в данном магазине увеличиваются примерно на 0.09 единиц.

Price (цена компании): Коэффициент перед этой переменной равен -0.091. Это указывает на то, что увеличение цены компании на автокресла на единицу приведет к уменьшению продаж на приблизительно 0.091 единицы.


Качество стеллажа для размещения автокресел влияет на их продажи: продажи кресел на хороших стеллажах были выше на 3005 штук, а на плохих были ниже на 1812 штук (относительно среднего качества стеллажа).

--------------------

Лабараторная работа №4 
"Методы снижения размерности. Регуляризация логистической регрессии"
В практических примерах ниже показано:

как снижать размерность пространства признаков методами главных компонент (PCR),частных наименьшах квадратов (PLS)

как строить логистическую регрессию с регуляризацией параметров (методы ридж и лассо)

Точность всех моделей оценивается методом перекрёстной проверки по 10 блокам.

Данные: https://raw.githubusercontent.com/aksyuk/MTML/main/Labs/data/default_of_credit_card_clients.csv


Задачи: 

1. Данные своего варианта (см. таблицу ниже) разделить на выборку для построения 
моделей (85%) и отложенные наблюдения (15%). Оставить в таблице только указанные в
варианте переменные. Отложенные наблюдения использовать только для прогноза по 
лучшей модели.

2. Провести предварительный и корреляционный анализ данных с помощью статистик и
графиков из этой лабораторной.

3. Снизить размерность пространства объясняющих переменных методом, указанным в
варианте. Обосновать количество главных компонент. Построить модель логистической 
регрессии на преобразованном пространстве объясняющих переменных. Оценить 
точность модели методом перекрёстной проверки.

4. Провести регуляризацию модели логистической регрессии методом, указанным в 
варианте. Подобрать оптимальное значение гиперпараметра методом перекрёстной
проверки. Построить график сжатия параметров модели.

5. Выбрать наиболее точную модель из полученных в пунктах 3 и 4, сделать по ней прогноз
на отложенные наблюдения, оценить точность этого прогноза.

РЕШЕНИЕ:

Загрузим все необходимые для работы модули, определим константы, напишем функцию, которая строит график сжатия коэффициентов в ридж и лассо. Загрузим входные данные.Определим количество строк и столбцов в наборе данных. Удалим ненужные переменные.

Число строк и столбцов в наборе данных:
 (30000, 24)

Тип данных: объекты

Предварительный анализ данных:

Считаем доли классов целевой переменной Y. Итак, всего целевых классов два, и их доли сильно разняться, с перевесом в пользу класса '0'. Все объясняющие переменные набора данных непрерывные. Рассчитаем для них описательные статистики.
Выводы по описательным статистикам: значения объясняющих переменных положительные кроме 'PAY_2', 'PAY_3' масштабы измерения отличаются. Для работы с методами снижения размерности и регуляризации понадобится стандартизация значений.
 
Визуализация разброса переменных внутри классов:

Построим коробчатые диаграммы для объясняющих переменных, чтобы сравнить средние уровни и разброс по классам.На графиках отличие в медианах и разбросе между классами прослеживается практически по всем объясняющим переменным. Меньше всего различаются коробчатые диаграммы попеременным "PAY_5","AGE","EDUCATION","PAY_6" . Это говорит о том, классы по зависимой переменной Y неплохо разделяются по всем объясняющим переменным.

Корреляционый анализ:

Теперь посмотрим на взаимодействие объясняющих переменных.Между объясняющими переменными обнаруживаются как прямые, так и обратные линейные взаимосвязи. Выведем все значимые коэффициенты в одной таблице и определим минимальный / максимальный из них.

Методы снижения размерности.

Регрессия на главные компоненты (PCR):

Пересчитаем объясняющие показатели в главные компоненты. Главные компоненты взаимно ортогональны, убедимся в этом и построим график объяснённой дисперсии.Столбцы на графике показывают долю исходной дисперсии исходных переменных, которую объясняет главная компонента. Линией показана накопленная доля. Так, видно, что первые 5 компонент объясняют 80% исходной дисперсии.Чтобы увидеть, как классы выглядят в координатах ГК на графике, придётся сократить пространство для двух компонент, которые объясняют 56% разброса объясняющих переменных. Видно, что в координатах двух компонент, рассчитанных методом частных наименьших квадратов, классы плохо разделимы. Теперь оценим точность модели с перекрёстной проверкой. Аcc = 0.96


Методы сжатия

Технически реализация лассо-регрессии отличается от ридж единственным аргументом penalty='l1' в функции LogisticRegression. Подбираем гиперпараметр регуляризации с помощью перекрёстной проверки. В функции LogisticRegression() есть аргумент – это инверсия гиперпараметра лямбда. Изобразим изменение коэффициентов ридж-регрессии на графике и сделаем отсечку на
уровне оптимального параметра. Итак, судя по графикам, для значения гиперпараметра, дающего самую точную модель, некоторые коэффициенты при объясняющих переменныхобнуляется. Это подтверждает наблюдение, сделанное нами ещё на этапе предварительного анализа: все объясняющие переменные плохо разделяют классы.

Прогноз на отложенные наблюдения по лучшей модели

Ещё раз посмотрим на точность построенных моделей.Все модели показывают высокую точность по показателю , при этом самой точной оказывается ридж-регрессия. Сделаем прогноз на отложенные наблюдения.Итак, методом логистической регрессии со сжатием коэффициенты с L2-регуляризацией мы получили идеально точную модель





